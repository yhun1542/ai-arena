import type { VercelRequest, VercelResponse } from '@vercel/node';

// Node.js ëŸ°íƒ€ì„ ê°•ì œ (ë²¤ë” SDK/HTTP ëª¨ë“ˆ í˜¸í™˜ì„±)
export const runtime = 'nodejs';

// AI ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
interface AIModel {
  id: string;
  name: string;
  provider: string;
  apiEndpoint: string;
  model: string;
  specialty: string;
  icon: string;
  color: string;
  maxTokens: number;
  temperature: number;
}

// AI ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
interface AIResponse {
  modelId: string;
  content: string;
  score: number;
  processingTime: number;
  confidence: number;
  strengths: string[];
  concerns: string[];
  metadata: {
    tokenCount: number;
    cost: number;
  };
}

// ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì„¤ì •
interface OrchestrationConfig {
  mode: 'parallel' | 'tournament' | 'debate' | 'consensus';
  selectedModels: string[];
  rounds: number;
  votingEnabled: boolean;
  timeoutMs: number;
  maxConcurrency: number;
}

// í™•ì¥ëœ AI ëª¨ë¸ ì •ì˜
const AI_MODELS: AIModel[] = [
  // ê¸°ì¡´ ëª¨ë¸ë“¤
  {
    id: 'gpt-4o',
    name: 'GPT-4o',
    provider: 'OpenAI',
    apiEndpoint: 'https://api.openai.com/v1/chat/completions',
    model: 'gpt-4o',
    specialty: 'ì¢…í•© ë¶„ì„',
    icon: 'ğŸ¤–',
    color: '#10B981',
    maxTokens: 4096,
    temperature: 0.7
  },
  {
    id: 'gemini-pro',
    name: 'Gemini Pro',
    provider: 'Google',
    apiEndpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent',
    model: 'gemini-pro',
    specialty: 'ì°½ì˜ì  ì‚¬ê³ ',
    icon: 'ğŸ’',
    color: '#3B82F6',
    maxTokens: 2048,
    temperature: 0.8
  },
  {
    id: 'claude-3-sonnet',
    name: 'Claude 3 Sonnet',
    provider: 'Anthropic',
    apiEndpoint: 'https://api.anthropic.com/v1/messages',
    model: 'claude-3-sonnet-20240229',
    specialty: 'ë…¼ë¦¬ì  ê²€ì¦',
    icon: 'ğŸ§ ',
    color: '#8B5CF6',
    maxTokens: 4096,
    temperature: 0.6
  },
  {
    id: 'grok-beta',
    name: 'Grok',
    provider: 'xAI',
    apiEndpoint: 'https://api.x.ai/v1/chat/completions',
    model: 'grok-beta',
    specialty: 'ì‹¤ìš©ì  ì ‘ê·¼',
    icon: 'âš¡',
    color: '#F59E0B',
    maxTokens: 2048,
    temperature: 0.7
  },
  // ìƒˆë¡œìš´ ëª¨ë¸ë“¤
  {
    id: 'mistral-large',
    name: 'Mistral Large',
    provider: 'Mistral AI',
    apiEndpoint: 'https://api.mistral.ai/v1/chat/completions',
    model: 'mistral-large-latest',
    specialty: 'íš¨ìœ¨ì„± ì „ë¬¸ê°€',
    icon: 'ğŸš€',
    color: '#EF4444',
    maxTokens: 4096,
    temperature: 0.7
  },
  {
    id: 'command-r-plus',
    name: 'Command R+',
    provider: 'Cohere',
    apiEndpoint: 'https://api.cohere.ai/v1/chat',
    model: 'command-r-plus',
    specialty: 'ê²€ìƒ‰ ì „ë¬¸ê°€',
    icon: 'ğŸ”',
    color: '#06B6D4',
    maxTokens: 4096,
    temperature: 0.7
  },
  {
    id: 'llama-3-1',
    name: 'Llama 3.1',
    provider: 'Meta',
    apiEndpoint: 'https://api.llama.com/v1/chat/completions',
    model: 'llama-3.1-70b-instruct',
    specialty: 'ì½”ë”© ì „ë¬¸ê°€',
    icon: 'ğŸ¦™',
    color: '#8B5A2B',
    maxTokens: 4096,
    temperature: 0.5
  },
  {
    id: 'perplexity-online',
    name: 'Perplexity AI',
    provider: 'Perplexity',
    apiEndpoint: 'https://api.perplexity.ai/chat/completions',
    model: 'llama-3.1-sonar-large-128k-online',
    specialty: 'íŒ©íŠ¸ì²´í‚¹ ì „ë¬¸ê°€',
    icon: 'ğŸ“Š',
    color: '#7C3AED',
    maxTokens: 4096,
    temperature: 0.6
  }
];

// AI ëª¨ë¸ë³„ API í˜¸ì¶œ í•¨ìˆ˜
async function callAIModel(model: AIModel, prompt: string, apiKeys: Record<string, string>): Promise<AIResponse> {
  const startTime = Date.now();
  
  try {
    let response: any;
    let content = '';
    
    switch (model.provider) {
      case 'OpenAI':
        response = await callOpenAI(model, prompt, apiKeys.OPENAI_API_KEY);
        content = response.choices[0]?.message?.content || '';
        break;
        
      case 'Google':
        response = await callGemini(model, prompt, apiKeys.GEMINI_API_KEY);
        content = response.candidates[0]?.content?.parts[0]?.text || '';
        break;
        
      case 'Anthropic':
        response = await callClaude(model, prompt, apiKeys.ANTHROPIC_API_KEY);
        content = response.content[0]?.text || '';
        break;
        
      case 'xAI':
        response = await callGrok(model, prompt, apiKeys.XAI_API_KEY);
        content = response.choices[0]?.message?.content || '';
        break;
        
      case 'Mistral AI':
        response = await callMistral(model, prompt, apiKeys.MISTRAL_API_KEY);
        content = response.choices[0]?.message?.content || '';
        break;
        
      case 'Cohere':
        response = await callCohere(model, prompt, apiKeys.COHERE_API_KEY);
        content = response.text || '';
        break;
        
      case 'Meta':
        response = await callLlama(model, prompt, apiKeys.LLAMA_API_KEY);
        content = response.choices[0]?.message?.content || '';
        break;
        
      case 'Perplexity':
        response = await callPerplexity(model, prompt, apiKeys.PERPLEXITY_API_KEY);
        content = response.choices[0]?.message?.content || '';
        break;
        
      default:
        throw new Error(`Unsupported provider: ${model.provider}`);
    }
    
    const processingTime = Date.now() - startTime;
    
    return {
      modelId: model.id,
      content,
      score: calculateScore(content, model.specialty),
      processingTime,
      confidence: calculateConfidence(content),
      strengths: extractStrengths(content, model.specialty),
      concerns: extractConcerns(content),
      metadata: {
        tokenCount: estimateTokenCount(content),
        cost: calculateCost(model, content)
      }
    };
    
  } catch (error) {
    console.error(`Error calling ${model.name}:`, error);
    
    return {
      modelId: model.id,
      content: `${model.name} ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : 'Unknown error'}`,
      score: 0,
      processingTime: Date.now() - startTime,
      confidence: 0,
      strengths: [],
      concerns: ['API í˜¸ì¶œ ì‹¤íŒ¨'],
      metadata: {
        tokenCount: 0,
        cost: 0
      }
    };
  }
}

// ê°œë³„ AI API í˜¸ì¶œ í•¨ìˆ˜ë“¤
async function callOpenAI(model: AIModel, prompt: string, apiKey: string) {
  const response = await fetch(model.apiEndpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: model.model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: model.maxTokens,
      temperature: model.temperature
    }),
  });
  
  if (!response.ok) {
    throw new Error(`OpenAI API error: ${response.status}`);
  }
  
  return response.json();
}

async function callGemini(model: AIModel, prompt: string, apiKey: string) {
  const response = await fetch(`${model.apiEndpoint}?key=${apiKey}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [{
        parts: [{ text: prompt }]
      }],
      generationConfig: {
        maxOutputTokens: model.maxTokens,
        temperature: model.temperature
      }
    }),
  });
  
  if (!response.ok) {
    throw new Error(`Gemini API error: ${response.status}`);
  }
  
  return response.json();
}

async function callClaude(model: AIModel, prompt: string, apiKey: string) {
  const response = await fetch(model.apiEndpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01'
    },
    body: JSON.stringify({
      model: model.model,
      max_tokens: model.maxTokens,
      temperature: model.temperature,
      messages: [{ role: 'user', content: prompt }]
    }),
  });
  
  if (!response.ok) {
    throw new Error(`Claude API error: ${response.status}`);
  }
  
  return response.json();
}

async function callGrok(model: AIModel, prompt: string, apiKey: string) {
  const response = await fetch(model.apiEndpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: model.model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: model.maxTokens,
      temperature: model.temperature
    }),
  });
  
  if (!response.ok) {
    throw new Error(`Grok API error: ${response.status}`);
  }
  
  return response.json();
}

async function callMistral(model: AIModel, prompt: string, apiKey: string) {
  const response = await fetch(model.apiEndpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: model.model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: model.maxTokens,
      temperature: model.temperature
    }),
  });
  
  if (!response.ok) {
    throw new Error(`Mistral API error: ${response.status}`);
  }
  
  return response.json();
}

async function callCohere(model: AIModel, prompt: string, apiKey: string) {
  const response = await fetch(model.apiEndpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: model.model,
      message: prompt,
      max_tokens: model.maxTokens,
      temperature: model.temperature
    }),
  });
  
  if (!response.ok) {
    throw new Error(`Cohere API error: ${response.status}`);
  }
  
  return response.json();
}

async function callLlama(model: AIModel, prompt: string, apiKey: string) {
  const response = await fetch(model.apiEndpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: model.model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: model.maxTokens,
      temperature: model.temperature
    }),
  });
  
  if (!response.ok) {
    throw new Error(`Llama API error: ${response.status}`);
  }
  
  return response.json();
}

async function callPerplexity(model: AIModel, prompt: string, apiKey: string) {
  const response = await fetch(model.apiEndpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: model.model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: model.maxTokens,
      temperature: model.temperature
    }),
  });
  
  if (!response.ok) {
    throw new Error(`Perplexity API error: ${response.status}`);
  }
  
  return response.json();
}

// ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
function calculateScore(content: string, specialty: string): number {
  // ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚° ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í‰ê°€ í•„ìš”)
  const baseScore = Math.min(90 + Math.random() * 10, 100);
  const lengthBonus = Math.min(content.length / 100, 5);
  return Math.round(baseScore + lengthBonus);
}

function calculateConfidence(content: string): number {
  // ì‹ ë¢°ë„ ê³„ì‚° (ê¸¸ì´, êµ¬ì²´ì„± ë“± ê¸°ë°˜)
  const lengthFactor = Math.min(content.length / 500, 1);
  const structureFactor = content.includes('##') || content.includes('**') ? 0.2 : 0;
  return Math.round((lengthFactor + structureFactor) * 100);
}

function extractStrengths(content: string, specialty: string): string[] {
  const strengths = [specialty];
  if (content.includes('ë¶„ì„')) strengths.push('ì²´ê³„ì  ë¶„ì„');
  if (content.includes('ì°½ì˜')) strengths.push('ì°½ì˜ì  ì ‘ê·¼');
  if (content.includes('ë…¼ë¦¬')) strengths.push('ë…¼ë¦¬ì  ì‚¬ê³ ');
  if (content.includes('ì‹¤ìš©')) strengths.push('ì‹¤ìš©ì  í•´ê²°');
  return strengths.slice(0, 3);
}

function extractConcerns(content: string): string[] {
  const concerns = [];
  if (content.length < 200) concerns.push('ì‘ë‹µ ê¸¸ì´ ë¶€ì¡±');
  if (!content.includes('##') && !content.includes('**')) concerns.push('êµ¬ì¡°í™” ë¶€ì¡±');
  if (content.includes('í™•ì‹¤í•˜ì§€ ì•Š')) concerns.push('ë¶ˆí™•ì‹¤ì„±');
  return concerns.slice(0, 2);
}

function estimateTokenCount(content: string): number {
  return Math.ceil(content.length / 4); // ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚°
}

function calculateCost(model: AIModel, content: string): number {
  const tokenCount = estimateTokenCount(content);
  // ëª¨ë¸ë³„ ëŒ€ëµì ì¸ ë¹„ìš© (ì‹¤ì œ ë¹„ìš©ì€ ë” ì •í™•í•œ ê³„ì‚° í•„ìš”)
  const costPerToken = {
    'gpt-4o': 0.00003,
    'gemini-pro': 0.000125,
    'claude-3-sonnet': 0.000015,
    'grok-beta': 0.00002,
    'mistral-large': 0.000008,
    'command-r-plus': 0.00003,
    'llama-3-1': 0.000001,
    'perplexity-online': 0.00002
  };
  
  return tokenCount * (costPerToken[model.id as keyof typeof costPerToken] || 0.00001);
}

// ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ëª¨ë“œë³„ ì²˜ë¦¬
async function processParallelMode(
  selectedModels: AIModel[],
  prompt: string,
  apiKeys: Record<string, string>,
  timeoutMs: number
): Promise<AIResponse[]> {
  const promises = selectedModels.map(model => 
    Promise.race([
      callAIModel(model, prompt, apiKeys),
      new Promise<AIResponse>((_, reject) => 
        setTimeout(() => reject(new Error('Timeout')), timeoutMs)
      )
    ]).catch(error => ({
      modelId: model.id,
      content: `íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì˜¤ë¥˜: ${error.message}`,
      score: 0,
      processingTime: timeoutMs,
      confidence: 0,
      strengths: [],
      concerns: ['íƒ€ì„ì•„ì›ƒ'],
      metadata: { tokenCount: 0, cost: 0 }
    }))
  );
  
  return Promise.all(promises);
}

async function processTournamentMode(
  selectedModels: AIModel[],
  prompt: string,
  apiKeys: Record<string, string>,
  rounds: number
): Promise<AIResponse[]> {
  let currentModels = [...selectedModels];
  let allResponses: AIResponse[] = [];
  
  for (let round = 0; round < rounds && currentModels.length > 1; round++) {
    const responses = await processParallelMode(currentModels, prompt, apiKeys, 30000);
    allResponses.push(...responses);
    
    // ìƒìœ„ 50% ëª¨ë¸ë§Œ ë‹¤ìŒ ë¼ìš´ë“œ ì§„ì¶œ
    responses.sort((a, b) => b.score - a.score);
    const survivorCount = Math.max(1, Math.ceil(currentModels.length / 2));
    const survivorIds = responses.slice(0, survivorCount).map(r => r.modelId);
    currentModels = currentModels.filter(m => survivorIds.includes(m.id));
  }
  
  return allResponses;
}

// ë©”ì¸ í•¸ë“¤ëŸ¬
export default async function handler(
  request: VercelRequest,
  response: VercelResponse,
) {
  // CORS í—¤ë” ì„¤ì •
  response.setHeader('Access-Control-Allow-Origin', '*');
  response.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
  response.setHeader('Access-Control-Allow-Headers', 'Content-Type');

  if (request.method === 'OPTIONS') {
    return response.status(200).end();
  }

  if (request.method !== 'POST') {
    return response.status(405).json({
      error: 'Method Not Allowed',
      message: 'Only POST requests are supported'
    });
  }

  try {
    const { 
      query, 
      config = {
        mode: 'parallel',
        selectedModels: ['gpt-4o', 'gemini-pro', 'claude-3-sonnet', 'grok-beta'],
        rounds: 1,
        votingEnabled: false,
        timeoutMs: 30000,
        maxConcurrency: 8
      }
    }: { 
      query: string; 
      config?: Partial<OrchestrationConfig> 
    } = request.body;

    // ì…ë ¥ ê²€ì¦
    if (!query || typeof query !== 'string') {
      return response.status(400).json({
        error: 'Invalid Input',
        message: 'Query is required and must be a string'
      });
    }

    if (query.length < 10 || query.length > 2000) {
      return response.status(400).json({
        error: 'Invalid Query Length',
        message: 'Query must be between 10 and 2000 characters'
      });
    }

    // API í‚¤ ìˆ˜ì§‘
    const apiKeys = {
      OPENAI_API_KEY: process.env.OPENAI_API_KEY || '',
      GEMINI_API_KEY: process.env.GEMINI_API_KEY || '',
      ANTHROPIC_API_KEY: process.env.ANTHROPIC_API_KEY || '',
      XAI_API_KEY: process.env.XAI_API_KEY || '',
      MISTRAL_API_KEY: process.env.MISTRAL_API_KEY || '',
      COHERE_API_KEY: process.env.COHERE_API_KEY || '',
      LLAMA_API_KEY: process.env.LLAMA_API_KEY || '',
      PERPLEXITY_API_KEY: process.env.PERPLEXITY_API_KEY || ''
    };

    // ì„ íƒëœ ëª¨ë¸ë“¤ í•„í„°ë§
    const selectedModels = AI_MODELS.filter(model => 
      config.selectedModels?.includes(model.id)
    );

    if (selectedModels.length === 0) {
      return response.status(400).json({
        error: 'No Valid Models',
        message: 'At least one valid model must be selected'
      });
    }

    console.log(`ğŸ­ AI Orchestra ìš”ì²­:`, {
      mode: config.mode,
      models: selectedModels.map(m => m.name),
      queryLength: query.length,
      timestamp: new Date().toISOString()
    });

    const startTime = Date.now();
    let responses: AIResponse[] = [];

    // ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ëª¨ë“œë³„ ì²˜ë¦¬
    switch (config.mode) {
      case 'parallel':
        responses = await processParallelMode(selectedModels, query, apiKeys, config.timeoutMs || 30000);
        break;
        
      case 'tournament':
        responses = await processTournamentMode(selectedModels, query, apiKeys, config.rounds || 2);
        break;
        
      case 'debate':
      case 'consensus':
        // í–¥í›„ êµ¬í˜„ ì˜ˆì •
        responses = await processParallelMode(selectedModels, query, apiKeys, config.timeoutMs || 30000);
        break;
        
      default:
        responses = await processParallelMode(selectedModels, query, apiKeys, config.timeoutMs || 30000);
    }

    const totalProcessingTime = Date.now() - startTime;

    // ê²°ê³¼ ì •ë ¬ (ì ìˆ˜ ê¸°ì¤€)
    responses.sort((a, b) => b.score - a.score);

    // ìµœì¢… ê²°ê³¼ êµ¬ì„±
    const result = {
      orchestration: {
        mode: config.mode,
        totalModels: selectedModels.length,
        successfulResponses: responses.filter(r => r.score > 0).length,
        averageScore: responses.reduce((sum, r) => sum + r.score, 0) / responses.length,
        totalCost: responses.reduce((sum, r) => sum + r.metadata.cost, 0),
        processingTime: `${(totalProcessingTime / 1000).toFixed(1)}ì´ˆ`
      },
      responses: responses.map(r => ({
        model: selectedModels.find(m => m.id === r.modelId),
        response: r
      })),
      summary: {
        winner: responses[0]?.modelId || 'none',
        consensus: responses.filter(r => r.score > 85).length > responses.length / 2,
        topInsights: responses
          .filter(r => r.score > 80)
          .slice(0, 3)
          .map(r => r.content.substring(0, 200) + '...')
      }
    };

    return response.status(200).json({
      success: true,
      data: result,
      metadata: {
        requestId: `orchestra-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
        timestamp: new Date().toISOString(),
        version: '2.0.0'
      }
    });

  } catch (error) {
    console.error('âŒ AI Orchestra ì˜¤ë¥˜:', error);

    return response.status(500).json({
      error: 'Internal Server Error',
      message: 'ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      code: 'ORCHESTRA_ERROR',
      timestamp: new Date().toISOString()
    });
  }
}
