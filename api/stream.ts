import type { VercelRequest, VercelResponse } from '@vercel/node';
import { streamText, CoreMessage } from 'ai';
import { openai } from '@ai-sdk/openai';
import { google } from '@ai-sdk/google';
import { anthropic } from '@ai-sdk/anthropic';
import { createOpenAI } from '@ai-sdk/openai';
import { v4 as uuidv4 } from 'uuid';

// Grok (xAI) μ„¤μ •: OpenAIμ™€ νΈν™λλ―€λ΅ baseURLμ„ xAI μ—”λ“ν¬μΈνΈλ΅ μ§€μ •
const grok = createOpenAI({
  apiKey: process.env.XAI_API_KEY || '',
  baseURL: 'https://api.x.ai/v1',
});

export default async function handler(
  request: VercelRequest,
  response: VercelResponse,
) {
  const requestId = uuidv4();
  
  // CORS ν—¤λ” μ„¤μ •
  response.setHeader('x-request-id', requestId);
  response.setHeader('Access-Control-Allow-Origin', '*');
  response.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
  response.setHeader('Access-Control-Allow-Headers', 'Content-Type');

  if (request.method === 'OPTIONS') {
    return response.status(200).end();
  }

  try {
    console.log('π€ Stream API μ”μ²­ μμ‹ :', {
      method: request.method,
      requestId,
      timestamp: new Date().toISOString()
    });

    let query: string;
    let provider: string = 'OPENAI';

    if (request.method === 'GET') {
      // GET μ”μ²­ - μΏΌλ¦¬ νλΌλ―Έν„°μ—μ„ λ°μ΄ν„° μ¶”μ¶
      query = request.query.q as string;
      provider = (request.query.provider as string) || 'OPENAI';
      
      if (!query) {
        return response.status(400).json({
          error: 'Bad Request',
          message: 'Query parameter "q" is required',
          requestId
        });
      }

    } else if (request.method === 'POST') {
      // POST μ”μ²­ - μ”μ²­ λ³Έλ¬Έμ—μ„ λ°μ΄ν„° μ¶”μ¶
      const { messages, provider: reqProvider, query: reqQuery } = request.body;
      
      if (reqQuery && typeof reqQuery === 'string') {
        query = reqQuery;
      } else if (messages && Array.isArray(messages) && messages.length > 0) {
        query = messages[messages.length - 1]?.content || '';
      } else {
        return response.status(400).json({
          error: 'Bad Request',
          message: 'Either "query" or "messages" is required',
          requestId
        });
      }
      
      provider = reqProvider || 'OPENAI';

    } else {
      return response.status(405).json({
        error: 'Method Not Allowed',
        message: 'Only GET and POST requests are supported',
        requestId
      });
    }

    // λ©”μ‹μ§€ ν•μ‹μΌλ΅ λ³€ν™
    const formattedMessages: CoreMessage[] = [
      {
        role: 'user',
        content: query
      }
    ];

    // μ κ³µμμ— λ”°λ¥Έ λ¨λΈ μ„ νƒ
    let model;
    let modelName = '';

    switch (provider.toUpperCase()) {
      case 'OPENAI':
      case 'GPT':
        if (!process.env.OPENAI_API_KEY) {
          return await handleFallbackStream(response, query, 'OpenAI', requestId);
        }
        model = openai('gpt-4o');
        modelName = 'GPT-4o';
        break;

      case 'GEMINI':
      case 'GOOGLE':
        if (!process.env.GOOGLE_GENERATIVE_AI_API_KEY) {
          return await handleFallbackStream(response, query, 'Gemini', requestId);
        }
        model = google('models/gemini-1.5-flash-latest');
        modelName = 'Gemini 1.5 Flash';
        break;

      case 'CLAUDE':
      case 'ANTHROPIC':
        if (!process.env.ANTHROPIC_API_KEY) {
          return await handleFallbackStream(response, query, 'Claude', requestId);
        }
        model = anthropic('claude-3-5-sonnet-20241022');
        modelName = 'Claude 3.5 Sonnet';
        break;

      case 'GROK':
      case 'XAI':
        if (!process.env.XAI_API_KEY) {
          return await handleFallbackStream(response, query, 'Grok', requestId);
        }
        model = grok('grok-beta');
        modelName = 'Grok Beta';
        break;

      default:
        return response.status(400).json({
          error: 'Invalid Provider',
          message: 'Supported providers: OPENAI, GEMINI, CLAUDE, GROK',
          supportedProviders: ['OPENAI', 'GEMINI', 'CLAUDE', 'GROK'],
          requestId
        });
    }

    console.log(`π¤– ${modelName} λ¨λΈλ΅ μ¤νΈλ¦¬λ° μ‹μ‘`);

    // μ¤νΈλ¦¬λ° μ‘λ‹µμ„ μ„ν• ν—¤λ” μ„¤μ •
    response.setHeader('Content-Type', 'text/plain; charset=utf-8');
    response.setHeader('Transfer-Encoding', 'chunked');
    response.setHeader('Cache-Control', 'no-cache');
    response.setHeader('Connection', 'keep-alive');

    // μ¤νΈλ¦¬λ° μ‹μ‘ λ©”μ‹μ§€
    response.write(`π¤– ${modelName}μ΄ "${query}"μ— λ€ν•΄ λ¶„μ„ μ¤‘μ…λ‹λ‹¤...\n\n`);

    // AI SDKμ streamText ν•¨μλ¥Ό μ‚¬μ©ν•μ—¬ μ¤νΈλ¦¬λ° μ‘λ‹µ μƒμ„±
    const result = await streamText({
      model: model,
      messages: formattedMessages,
      temperature: 0.7,
      maxTokens: 1500,
    });

    // μ¤νΈλ¦¬λ° μ‘λ‹µ μ²λ¦¬
    for await (const textPart of result.textStream) {
      response.write(textPart);
    }

    // μ¤νΈλ¦¬λ° μ™„λ£ λ©”μ‹μ§€
    response.write(`\n\n---\nπ“ μ”μ²­ ID: ${requestId}\nπ¤– λ¨λΈ: ${modelName}\nβ° μ™„λ£ μ‹κ°„: ${new Date().toISOString()}\n`);
    
    // μ¤νΈλ¦¬λ° μ™„λ£
    response.end();
    console.log(`β… ${modelName} μ¤νΈλ¦¬λ° μ™„λ£`);

  } catch (error) {
    console.error('β Stream API μ¤λ¥:', error);

    // μ΄λ―Έ μ‘λ‹µμ΄ μ‹μ‘λ κ²½μ° μ²λ¦¬
    if (response.headersSent) {
      if (!response.writableEnded) {
        response.write(`\n\nβ μ¤νΈλ¦¬λ° μ¤‘ μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤: ${error instanceof Error ? error.message : 'Unknown error'}\n`);
        response.end();
      }
      return;
    }

    // μ—λ¬ νƒ€μ…λ³„ μ²λ¦¬
    if (error instanceof Error) {
      if (error.message.includes('API key')) {
        return response.status(503).json({
          error: 'Service Configuration Error',
          message: 'AI μ„λΉ„μ¤ μ„¤μ •μ— λ¬Έμ κ°€ μμµλ‹λ‹¤.',
          code: 'CONFIG_ERROR',
          requestId
        });
      }

      if (error.message.includes('rate limit') || error.message.includes('quota')) {
        return response.status(429).json({
          error: 'Rate Limit Exceeded',
          message: 'μ”μ²­μ΄ λ„λ¬΄ λ§μµλ‹λ‹¤. μ μ‹ ν›„ λ‹¤μ‹ μ‹λ„ν•΄μ£Όμ„Έμ”.',
          code: 'RATE_LIMIT',
          requestId
        });
      }
    }

    // μΌλ°μ μΈ μ„λ²„ μ¤λ¥
    return response.status(500).json({
      error: 'Internal Server Error',
      message: 'μ¤νΈλ¦¬λ° μ²λ¦¬ μ¤‘ μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤.',
      code: 'STREAM_ERROR',
      requestId,
      timestamp: new Date().toISOString()
    });
  }
}

// Fallback μ¤νΈλ¦¬λ° ν•¨μ (API ν‚¤κ°€ μ—†μ„ λ•)
async function handleFallbackStream(
  response: VercelResponse,
  query: string,
  modelName: string,
  requestId: string
) {
  // μ¤νΈλ¦¬λ° μ‘λ‹µμ„ μ„ν• ν—¤λ” μ„¤μ •
  response.setHeader('Content-Type', 'text/plain; charset=utf-8');
  response.setHeader('Transfer-Encoding', 'chunked');
  response.setHeader('Cache-Control', 'no-cache');
  response.setHeader('Connection', 'keep-alive');

  response.write(`π¤– ${modelName} μ‹λ®¬λ μ΄μ… λ¨λ“λ΅ "${query}"μ— λ€ν•΄ λ¶„μ„ μ¤‘μ…λ‹λ‹¤...\n\n`);

  const fallbackResponse = `μ•λ…•ν•μ„Έμ”! ${modelName} AIμ…λ‹λ‹¤.

ν„μ¬ μ§λ¬Έ: "${query}"

μ£„μ†΅ν•©λ‹λ‹¤. ν„μ¬ ${modelName} API ν‚¤κ°€ μ„¤μ •λμ§€ μ•μ•„ μ‹λ®¬λ μ΄μ… λ¨λ“λ΅ λ™μ‘ν•©λ‹λ‹¤.

π“ μ§λ¬Έ λ¶„μ„:
- μ§λ¬Έ μ ν•: ${query.includes('?') ? 'μ§μν•' : 'μ„μ ν•'}
- μ§λ¬Έ κΈΈμ΄: ${query.length}μ
- μ–Έμ–΄: ${/[γ„±-γ…|γ…-γ…£|κ°€-ν£]/.test(query) ? 'ν•κµ­μ–΄' : 'μμ–΄'}

π”§ ${modelName} νΉμ§•:
${getModelCharacteristics(modelName)}

μ‹¤μ  API ν‚¤κ°€ μ„¤μ •λλ©΄ λ”μ± μ •ν™•ν•κ³  μƒμ„Έν• λ‹µλ³€μ„ μ κ³µν•  μ μμµλ‹λ‹¤.`;

  // μ¤νΈλ¦¬λ° μ‹λ®¬λ μ΄μ…
  const chunks = fallbackResponse.split('\n');
  for (const chunk of chunks) {
    response.write(chunk + '\n');
    await new Promise(resolve => setTimeout(resolve, 50));
  }

  response.write(`\n\n---\nπ“ μ”μ²­ ID: ${requestId}\nπ¤– λ¨λΈ: ${modelName} (μ‹λ®¬λ μ΄μ…)\nβ° μ™„λ£ μ‹κ°„: ${new Date().toISOString()}\n`);
  response.end();
}

function getModelCharacteristics(modelName: string): string {
  switch (modelName) {
    case 'OpenAI':
      return '- μΆ…ν•©μ μ΄κ³  μ²΄κ³„μ μΈ λ¶„μ„\n- κ· ν•μ΅ν κ΄€μ  μ μ‹\n- μƒμ„Έν• μ„¤λ…κ³Ό μμ‹ μ κ³µ';
    case 'Gemini':
      return '- μ°½μμ μ΄κ³  νμ‹ μ μΈ μ ‘κ·Ό\n- λ‹¤κ°λ„ λ¶„μ„κ³Ό ν†µμ°°\n- μ‹κ°μ  μ •λ³΄ μ²λ¦¬ λ¥λ ¥';
    case 'Claude':
      return '- λ…Όλ¦¬μ μ΄κ³  μ¤λ¦¬μ μΈ λ¶„μ„\n- κ· ν•μ΅ν λΉ„νμ  μ‚¬κ³ \n- μ•μ „ν•κ³  μ‹ μ¤‘ν• λ‹µλ³€';
    case 'Grok':
      return '- μ‹¤μ©μ μ΄κ³  μ§μ„¤μ μΈ μ ‘κ·Ό\n- ν„μ‹¤μ μΈ κ΄€μ  μ μ‹\n- κ°„κ²°ν•κ³  λ…ν™•ν• λ‹µλ³€';
    default:
      return '- κ³ ν’μ§ AI λ¶„μ„ μ κ³µ\n- μ‚¬μ©μ λ§μ¶¤ν• λ‹µλ³€\n- μ •ν™•ν•κ³  μ μ©ν• μ •λ³΄';
  }
}
